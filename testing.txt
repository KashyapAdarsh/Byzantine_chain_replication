Team : 
	Adarsh Kashyap
	Kanishta Agarwal

Professor :
	Scott Stoller
CSE535 Course Project - Byzantine Chain Replication Implementation [http://www.cs.cornell.edu/~ns672/publications/2012OPODIS.pdf]

Team : 
	Adarsh Kashyap
	Kanishta Agarwal

Professor :
	Scott Stoller

In order to test multiple scenario's for our code we have written seperate config files

NOTE : Most of the testcases check for things like order proof verification, result proof verification, signing the statements and
	   hashing the results. So we do not have any specific test case to test them.

-----------
Performance Evalution - All working case
-----------

Raft2.da - elapsed time (seconds):  6.6313230991363525

Our Implementation:
	single host - 16.283s
	multiple host - 19.163s

----------
Scenario 1 - All working case
----------
	default.config : This is the default config if we do not specify any command line arguments

	Details :
		Number of clients : 3
		No of replicas	  : 3 [t = 1]
		One replica and one client per node.
		Failure injection : None

	Example command to use this testcase :

		python -m da --message-buffer-size 640000 -n main main.da [ In main termainal, see README for complete execution commands]

	Result :
		No errors of failures and all operations will be executed properly.

----------
Scenario 2	- All working case
----------
	default_t2.config : This is to test if our system works for more replicas

	Details :
		Number of clients : 3
		No of replicas	  : 5 [t = 2]
		One replica and one client per node.
		Failure injection : None

	Example command to use this testcase :

		python -m da --message-buffer-size 640000 -n main main.da ../config/default_t2.config

	Result :
		No errors of failures and all operations will be executed properly.


----------
Scenario 3	- Stress test
----------
	stress_test.config : This testcase is a stress test for our system

	Details :
		Number of clients : 20
		No of replicas	  : 11 [t = 5]
		Total number of messages : More than 300
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : None

	Example command to use this testcase :

		python -m da --message-buffer-size 640000 -n main main.da ../config/stress_test.config

	Result :
		No failures and all operations will be executed properly.
		There might be some errors with client verification at the end of testcase since clients are not 
		independent and might have modified each others data.

----------
Scneario 4	- Catchup Config test 
----------
	catchup.config : On receipt of m'th catch_up message on a specified replica, we will trigger the failure
	
	details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randomly spread across 3 nodes.
		Failure injection : shuttle(2,4),change_operation();catch_up(1),drop()
		Client timeout	  : 1000 seconds
		Other changes	  : To test this we first trigger failure as shuttle(2,4) and then on cathcup() request we trigger the second failure .

	Example command to use this testcase :

		python -m da --message-buffer-size 640000 -n main main.da ../config/catchup.config

	Result : 
		In our case we inject drop as failure on catchup trigger and thus olympus will choose a different quorum


----------
Scneario 5	- Checkpoint_crash test 
----------
	checkpoint_crash.config : On receipt of m'th checkpoint message on specified replica we trigger and inject failure
	
	details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randomly spread across 3 nodes.
		Failure injection : checkpoint(2),crash()
		Client timeout	  : 10 seconds
		Other changes	  : None

	Example command to use this testcase :

		python -m da --message-buffer-size 640000 -n main main.da ../config/checkpoint_crash.config

	Result : 
		On receipt of m'th checkpoint message we inject crash as failure 

----------
Scenario 6	- client Request Change Result test
----------
	client_req_change_res.config : On receipt of m'th request message directly from client c we inject the failure 

	details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randomly spread across 3 nodes.
		Failure injection : client_request(2,2), change_result()
		Client timeout	  : 1000 seconds
		Other changes	  : None

	Example command to use this testcase :

		python -m da --message-buffer-size 640000 -n main main.da ../config/client_req_change_res.config

	Result : 
		 On receipt of 2nd request message at replica 1 directly from client 2 we inject the failure change_result() which causes
		 a reconfiguration through olympus
		
----------
Scenario 7	- Client Request Change Operation Test
----------
	client_request_change_operation.config : This testcase is used to test client request trigger and change operation operation.

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : change operation
		Failure operation : client request

	Example command to use this testcase :

		python -m da --message-buffer-size 640000 -n main main.da ../config/client_request_change_operation.config

	Result :
		On receipt of m'th request message directly from client c we inject change operation failure. Olympus then reconfigures upon request
		Logs can be followed easily to understand the same.

----------
Scenario 8	- Completed Checkpoint Sleep
----------
	test_completed_checkpoint_sleep.config : This testcase is used to test completed_checkpoint(m) trigger and sleep operation

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : completed checkpoint
		Failure operation : sleep

	Example command to use this testcase :

		python -m da --message-buffer-size 640000 -n main main.da ../config/test_completed_checkpoint_sleep.config

	Result : On receipt of m'th completed checkpoint message we inject sleep operation


----------
Scenario 9	- Drop Checkpoint Stmts
----------
	drop_checkpoint_stmts.config : This test case is used to test completed_checkpoint trigger and drop_checkpoint_stmts operation.

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : completed_checkpoint
		Failure operation : drop_checkpoint_stmts

	Result : 
		On receipt of m'th completed checkpoint message we inject drop_checkpt_stmts failure

----------
Scenario 10	- Extraop Drop Result Stmt
----------
	extraop_drop_result_stmt.config : This test case is used to test extra_op() and drop_checkpoint_stmts().

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : extra_op(), drop_checkpoint_stmts()
		Failure operation : shuttle,completed_checkpoint

	Result : 
		On receipt of m'th shuttle message for a request by client c we inject extraop() and then receipt of m'th completed checkpoint message we inject drop_checkpoint_stmts failure.

-----------
Scenario 11 - forwarded_request_change_operation
-----------

	forwarded_request_change_operation.config : This test case is used to test forwarded_request trigger and change_operataion operation.

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : forwarded_request
		Failure operation : change_operation

	Result : 
		On receipt of m'th forwarded request message containing a request from client c we inject change_operation() failure

-----------
Scenario 12 - head_msg_seen
-----------

	head_msg_seen.config : This testcase is used to test retransmission case.

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.

	Result : 
		Client times out before result shuttle goes back up the chain,retransmission is sent to everyone and since none of them have result cached
		everyone forwards the request to head and starts timer, and now head has seen the msg_id so it does not start a new shuttle but instead waits, and now when the result shuttle comes back all replicas store the result and send result to client..


-----------
Scenario 13 - forwarded_request_change_operation
-----------

	forwarded_request_change_operation.config : This test case is used to test forwarded_request trigger and change_operataion operation.

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : forwarded_request
		Failure operation : change_operation

	Result : 
		On receipt of m'th forwarded request message containing a request from client c we inject change_operation() failure

-----------
Scenario 14 - invalid_order_result_sign
-----------

	invalid_order_result_sign.config : This test case is used to test invalid_order_sig() and invalid_result_sig() operation.

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : client_request, result_shuttle
		Failure operation : invalid_result_sig, invalid_order_sig

	Result : 
		On receipt of m'th request message directly from client c we invalid_order_sig() then on reconfiguration
		On receipt of m'th result-shuttle message for a request by client c we inject invalid_result_sig() failure

-----------
Scenario 15 - forwarded_request_change_operation
-----------

	forwarded_request_change_operation.config : This test case is used to test forwarded_request trigger and change_operataion operation.

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : forwarded_request
		Failure operation : change_operation

	Result : 
		On receipt of m'th forwarded request message containing a request from client c we inject change_operation() failure


-----------
Scenario 16 - shuttle_change_result
-----------

	shuttle_change_result.config : This test case is used to test shuttle trigger and change_result operation.

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : shuttle
		Failure operation : change_result

	Result : 
		Soon after we receive 'm'th result shuttle, from the replica we perform change result statement [to hash(OK)]and the replica before
		that will identify this, also if we perform this in tail then even client will identify that something is wrong.
Logs can be followed easily to undestand the same.


----------
Scenario 17	- result_shuttle_change_result
----------
	result_shuttle_change_result.config : This test case is used to test result_shuttle trigger and change_result operation.

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : resut_shuttle
		Failure operation : change_result

	Result : 
		Soon after we receive 'm'th result shuttle, from the replica 'r' we perform change result statement [to hash(OK)]and the replica before
		that will identify this, also if we perform this in tail then even client will identify that something is wrong.
Logs can be followed easily to undestand the same.


----------
Scenario 18	- result_shuttle_drop
----------
	result_shuttle_drop.config : This testcase is used to test result_shuttle trigger and drop_result_stmt operation.

	Details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randonly spread across 3 nodes.
		Failure injection : result_shuttle
		Failure operation : drop head statement

	Example command to use this testcase :

		python -m da --message-buffer-size 640000 -n main main.da ../config/stress_test.config

	Result :
		Soon after we receive 'm' result shuttle, from the replica 'r' we drop head statement and the replica before
		that will identify this, also if we perform this in tail then even client will identify that something is wrong.
Logs can be followed easily to undestand the same.

----------
Scenario 19	- test_saved_cache  
----------
	test_saved_cache.config : In this we test a scenario when client times out and requests all replicas,
							  now replicas will have this result cached, so everyone responds directly to client directly.

	details :
		Number of clients : 1
		No of replicas	  : 3 [t = 1]
		Replicas and clients are randomly spread across 3 nodes.
		Failure injection : None
		Client timeout	  : 8 seconds
		Other changes	  : To test this we stop tail from sending response to client.

	Example command to use this testcase :

		python -m da --message-buffer-size 640000 -n main main.da ../config/test_saved_cache.config

	Result : 
		All clients will receive result for all operations as a part of retransmission, can verify from logs that 
		saved cache is sent from replicas
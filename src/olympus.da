from logger import *
from classes import *
from constants import *
from replica import Replica
from util import *
import uuid
import time
import sys
import nacl.hash
import nacl.encoding
import nacl.signing
import pickle
from itertools import combinations

class Olympus(process):

	def setup(init_config):
		self.init_config = init_config
		self.client_config = dict()
		self.clients = dict()
		self.client_list = []
		self.signing_keys = dict()
		self.verify_keys = dict()
		self.olympus_id = uuid.uuid4()
		self.configs = []
		self.replica_list = []
		self.active_config = None
		self.current_slot = 0
		self.wedged_data = dict()
		self.data_store = dict()
		self.quorums = set()
		self.is_config_in_progress = False
		self.head_timeout = init_config[HEAD_TIMEOUT]
		self.nonhead_timeout = init_config[NONHEAD_TIMEOUT]
		self.caughtup_hash = dict()
		self.latest_result = dict()
		self.running_state_data = dict()
		self.head = None
		self.tail = None
		self.no_of_replicas = (2 * float(init_config[TOLERENCE])) + 1 # 2t+1
		self.no_of_clients = float(init_config[CLIENT_COUNT])
		self.all_replicas_spawned = False
		self.privateKey = None
		self.HASHER = nacl.hash.sha256
		self.reconfiguring_replicas_count = 0
		self.reconfiguring_replicas = False
		self.INFO = "[OLYMPUS] "
		self.failure_config_map = dict()
		log.info(self.INFO + "Olympus setup complete, assigned ID :" + str(self.olympus_id))

	def run():
		log.info(self.INFO + "Running test case : " + str(init_config["test_case_name"]))

		make_new_replica()

		# Wait untill all the clients register with olympus
		await(len(self.clients) == self.no_of_clients)

		# Sends client info to everyone
		for replica in self.replica_list:
			send(("client_details", self.clients, self.current_slot), to=replica)

		# Tell clients that replicas are ready and also send current 
		# active configuration details.
		for client in self.client_list:
			send(("replicas_ready", active_config),to=client)

		##populating the failure_config_map
		for key in list(self.init_config.keys()):
			if (key.startswith("failures[")):
				fail = self.init_config[key]
				failure_key = key

				config_num = int(failure_key.split(',')[0].lstrip('failures['))
				replica_index = int(failure_key.split(',')[1].strip().replace(']',''))

				all_failures = fail.split(';')
				shuttle_type_map = dict()

				for failure in all_failures:
					failure = failure.split(',')
					
					if (len(failure) == 2):
						client_index = None
						shuttle_type = failure[0].split('(')[0].strip()
						msg_count = int(failure[0].split('(')[1].replace(')','').strip())

						if ((client_index and client_index > len(self.client_list)) or replica_index > len(self.replica_list)):
							log.error(self.INFO +"Error processing failure injection.")
							return
						
						shuttle_key = failure[0].strip()

						if shuttle_key not in shuttle_type_map:
							opt = []
							opt_arg = []
							opt.append(failure[1].replace(')','').split('(')[0].strip())
							
							if (failure[1].replace(')','').split('(')[1].strip() == ''):
								opt_arg.append(None)
							else:
								opt_arg.append(failure[1].replace(')','').split('(')[1].strip())
							
							failure_config = FailureConfig(None, msg_count, shuttle_type, opt, opt_arg)
							shuttle_type_map[shuttle_key] = failure_config

						else:
							shuttle_type_map[shuttle_key].opt_type.append(failure[1].replace('(','').replace(')','').strip())
							
							if (failure[1].replace(')','').split('(')[1].strip() == ''):
								shuttle_type_map[shuttle_key].opt_arg.append(None)
							else:
								shuttle_type_map[shuttle_key].opt_arg.append(failure[1].replace(')','').split('(')[1].strip())


					elif(len(failure) == 3):
						shuttle_type = failure[0].split('(')[0].strip()
						msg_count = int(failure[1].replace(')', '').strip())
						client_index = int(failure[0].split('(')[1].strip())

						if ((client_index and client_index > len(self.client_list)) or replica_index > len(self.replica_list)):
							log.error(self.INFO +"Error processing failure injection.")
							return
						
						shuttle_key = failure[0].strip() + ',' + str(msg_count) + ')'

						if shuttle_key not in shuttle_type_map:
							opt = []
							opt_arg = []
							opt.append(failure[2].replace('(','').replace(')','').strip())
							if (failure[2].replace(')','').split('(')[1].strip() == ''):
								opt_arg.append(None)
							else:
								opt_arg.append(failure[2].replace(')','').split('(')[1].strip())
							
							failure_config = FailureConfig(self.client_list[client_index-1], \
																msg_count, shuttle_type, opt, opt_arg)

							shuttle_type_map[shuttle_key] = failure_config
						else:
							shuttle_type_map[shuttle_key].opt_type.append(failure[2].replace('(','').replace(')','').strip())
							
							if (len(failure[1].replace(')','').split('(')) == 1):
								shuttle_type_map[shuttle_key].opt_arg.append(None)
							else:
								shuttle_type_map[shuttle_key].opt_arg.append(failure[1].replace(')','').split('(')[1].strip())

					'''
					print("########### FAILURE CONFIG #############")
					print("msg count : ",shuttle_type_map[shuttle_key].msg_count)
					print("client ID : ",shuttle_type_map[shuttle_key].client_id)
					print("type :", shuttle_type_map[shuttle_key].shuttle_type)
					print("operations : ", shuttle_type_map[shuttle_key].opt_type)
					print("opt arg : ", shuttle_type_map[shuttle_key].opt_arg)
					'''
					
				self.failure_config_map[config_num] = [replica_index,shuttle_type_map]

		# Send fault injection to everyone
		send_fault_injection()

		# We need to keep olympus alive
		is_alive = False
		await(is_alive == True)

	def make_new_replica():
		config_id = uuid.uuid4()
		spawn_replicas()
		# This will help to keep slot number cont during reconfig
		log.info(self.INFO + "CURRENT CONFIG : " + str(config_id))
		configs.append(config_id)
		active_config = config_id

	def send_fault_injection():
		if reconfiguring_replicas_count in failure_config_map:
			replica_index, shuttle_type_map = failure_config_map[reconfiguring_replicas_count]

			log.info(self.INFO + "Sending failure config to " + str(self.replica_list[replica_index-1]))
		
			failure_config_list = list()

			for key,val in shuttle_type_map.items():
				failure_config_list.append(val)

			send(("failure_config_list",failure_config_list), to=self.replica_list[replica_index-1])


	# This method spawns required number of replica and it handles two cases
	# 1) If we are spawning for the first time
	# 2) If we are reconfiguring
	def spawn_replicas():
		if not self.replica_list:
			log.info(self.INFO + "Creating new replicas.")

			replica_hosts = self.init_config[REPLICA_HOSTS].split(';')

			# Running clients on different hosts
			for i in range(int(self.no_of_replicas)):
				node = 'Node' + str(replica_hosts[i])
				self.replica_list.append(new(Replica, agrs=(), at=node))

			self.replica_list = [next(iter(replica)) for replica in self.replica_list]

			generate_keys()

			for index in range(len(self.replica_list)):
				replica_config = make_replica_config(replica_list[index], \
											signing_keys[replica_list[index]])
				replica_config = add_head_tail(index, replica_config)
				
				# Add some details to replica config
				replica_config.head_id = self.replica_list[0]
				replica_config.head_timeout = self.head_timeout
				replica_config.nonhead_timeout = self.nonhead_timeout

				# sending the reconfiguration count to replica, to facilitate appropriate trigger of failure  
				if self.reconfiguring_replicas == True:
					replica_config.new_config_count = self.reconfiguring_replicas_count

				setup(self.replica_list[index],[replica_config])
				start(self.replica_list[index])

			self.all_replicas_spawned = True
			
			replica_chain = ""
			for replica in self.replica_list:
				replica_chain += str(replica) + '-->'
			log.info(self.INFO + "Chain---> " + replica_chain)			

		else:
			log.info(self.INFO + "Deleting old replicas.")
			for replica in self.replica_list:
				send(("exit"), to=replica)
			self.replica_list = []
			
			# making boolean reconfiguration == True and incrementing reconfig count, to handle failure injection on replicas
			self.reconfiguring_replicas = True
			self.reconfiguring_replicas_count += 1

			log.info(self.INFO + "Starting new set of processes for reconfiguration.")
			spawn_replicas()
			# Sends client info to everyone after re-spawning them
			for replica in self.replica_list:
				send(("client_details", self.clients, self.current_slot), to=replica)

			# Sends new fault config info to appropriate replica after re-spawning them
			send_fault_injection()

	# This method will generate signing keys and encoded verification keys for
	# all the replicas. We save it into two seperate dict.
	# <replica_id, signing key> and <replica_id, verification key>

	def generate_keys():
		log.info(self.INFO + "Generating signing and verification keys.")
		self.verify_keys = dict()
		
		for replica in self.replica_list:
			signing_key = nacl.signing.SigningKey.generate()
			verify_key = signing_key.verify_key
			verify_key_hex = verify_key.encode(encoder=nacl.encoding.HexEncoder)
			signing_keys[replica] = signing_key
			self.verify_keys[replica] = verify_key_hex

	# This method creates a configuration object for a replica
	# @arg - specific signing key, that will be sent only to this replica

	def make_replica_config(id, signing_key):
		replica_config = ReplicaConfig(id, self.verify_keys, signing_key,\
						 self.data_store, ACTIVE, self.replica_list, self.init_config[TESTCASE_NAME], \
						 	int(self.init_config["checkpoint_num"]), int(reconfiguring_replicas_count + 1))
		return replica_config

	# This method adds head and tail info for every replicaconfig object created.
	# We add next and prev replica details for the current replica.
	# we also set is_head and is_tail for current replica here

	def add_head_tail(replica_index, replica_config):
		if (replica_index == 0):
			log.info(self.INFO + "Head is " + str(replica_list[replica_index]))
			replica_config.is_head = True
			self.head = self.replica_list[replica_index]
			# If we have only one replica, then both head and tail are same
			if (len(self.replica_list) == 1):
				log.info(self.INFO + "Tail is " + str(replica_list[replica_index]))
				replica_config.is_tail = True
				self.tail = self.replica_list[replica_index]
			replica_config.next_replica_id = self.replica_list[replica_index + 1]
		elif (replica_index == len(self.replica_list) - 1):
			log.info(self.INFO + "Tail is " + str(replica_list[replica_index]))
			replica_config.is_tail = True
			self.tail = self.replica_list[replica_index]
			replica_config.prev_replica_id = self.replica_list[replica_index - 1]
		else:
			replica_config.next_replica_id = self.replica_list[replica_index + 1]
			replica_config.prev_replica_id = self.replica_list[replica_index - 1]

		return replica_config

	def hash(result):
		digest = self.HASHER(pickle.dumps(result), encoder=nacl.encoding.HexEncoder)
		return digest


	def verify_client_proof(proof):
		return True

	# After catch up response is received, we need to send any pending response to
	# the client.
	def send_pending_result_to_client(quorum):
		latest_per_client = dict()
		log.info(self.INFO + "Sending pending results to the clients.")
		for replica in quorum:
			if (replica not in list(self.latest_result.keys())):
				continue

			result = self.latest_result[replica]
			# Instead of sending everytime need to optimize here
			for client in list(result.keys()):
				response = ResponseObject(result[client][0].operation.msg_id, result[client][1], None)
				send(("result_from_olympus", response), to=client)

	def is_valid_quorum(quorum):
		log.info(self.INFO + "Replicas in current quorum :  " + str(quorum))

		# Lets check if checkpoint is same for all replicas in the list.
		# This can be optimized by identifying replica with issue and taking quorum that 
		# does not contain this replica.
		temp = dict() # Contains slot and freq.
		log.info(self.INFO + "Checking if checkpoint details are consistent.")
		for replica in quorum:
			for entry in self.wedged_data[replica].checkpoint:
				if (entry.slot in temp):
					temp[entry.slot] += 1
				else:
					temp[entry.slot] = 1

		print("checkpoint map : ", temp)

		# Meaning no checkpoint has happened before reconfig if len == 0
		if (len(temp) != 0 and len(set(temp.values()))!=1):
			log.info(self.INFO + "quorum validity failed, checkpoint is not consistent")
			return False, None, None

		log.info(self.INFO + "Checkpoint is consistent.")
		# Get last slot in the checkpoint and history should start after this checkpoint

		slot_to_start_history = 0
		if (len(temp) > 0):
			slot_to_start_history = self.wedged_data[quorum[0]].checkpoint[-1].slot
			print("History needs to start from : ", slot_to_start_history)

		# Lets check if all history starts with slot after checkpoint.
		longest_history_len = 0
		longest_replica = None

		log.info(self.INFO + "Checking if history details are consistent.")
		for replica in quorum:
			if (len(self.wedged_data[replica].history) == 0):
				return True, None, None

			if (self.wedged_data[replica].history[0].slot != slot_to_start_history + 1):
				log.info(self.INFO + "quorum validity failed, history starting slot does not match checkpoint")
				return False, None, None

			l = len(self.wedged_data[replica].history)
			if (l > longest_history_len):
				longest_replica = replica
				longest_history_len = l

		count = 0
		catch_up_replicas = set()
		while (count < longest_history_len):
			slot = None
			operation = None
			print('--------------')
			for replica in quorum:
				if (len(self.wedged_data[replica].history) > count):
					s = self.wedged_data[replica].history[count].slot
					o = self.wedged_data[replica].history[count].operation
					print("replica : ",replica ," slot : ", s, " type : ", o.type)
					if (slot == None and operation == None):
						slot = s 
						operation = o
					else:
						if (slot != s or operation.type != o.type or operation.msg_id != o.msg_id):
							log.info(self.INFO + "quorum validity failed, history is not consistent")
							return False, None, None
				else:
					catch_up_replicas.add(replica)

			count += 1

		log.info(self.INFO + "History is consistent.")
		return True, list(catch_up_replicas), longest_replica


	def get_running_state(quorum):
		log.info(self.INFO + "Catch up successfull, getting the running state.")
		for replica in quorum:
			log.info(self.INFO + "Asking replica " + str(replica) + " for running state.")
			send(("running_state"), to=replica)

		if(await(len(self.running_state_data) == 2)):
			temp = set()
			for data in list(self.running_state_data.values()):
				temp.add(hash(sorted(data.items())))

			if (len(temp) == 1):
				log.info(self.INFO + "Running state verified.")
				log.info(self.INFO + "Running state : " + str(self.running_state_data[quorum[0]]))
				self.data_store = data
				clear_data()
				make_new_replica()
				self.is_config_in_progress = False	
			else:
				if (len(self.quorums) > 0):
					self.running_state_data = dict()
					log.info(self.INFO + "Running state verification failed.")
					log.info(self.INFO + "Current quorum failed, getting next one.")
					start_quorum_check()
				else:
					log.info(self.INFO + "No valid quorums found.")
		elif (timeout(10)):
			log.info(self.INFO + "TIMEOUT, getting next quorum")
			if (len(self.quorums) > 0):
					self.running_state_data = dict()
					start_quorum_check()
			else:
				log.info(self.INFO + "No valid quorums found.")

	def clear_data():
		self.wedged_data = dict()
		self.quorums = set()
		self.caughtup_hash = dict()
		self.latest_result = dict()
		self.running_state_data = dict()

	def start_quorum_check():
		quorum = self.quorums.pop()
		res, catch_up_replicas, longest_replica = is_valid_quorum(quorum)

		if (res == True):

			if (longest_replica == None):
				log.info(self.INFO + "No history recorded yet, making new replicas")
				make_new_replica()
				return

			log.info(self.INFO + "Valid quorum found and checking if we need to catch up the history.")
			# if catch up list is empty we don't need to catchup
			if (len(catch_up_replicas) > 0):
				log.info(self.INFO + str(longest_replica) + " has more history, we need to catchup.")

				# Send a catch up message to all replicas in the list
				for replica in catch_up_replicas:
					send(("catch_up", self.wedged_data[longest_replica].history), to=replica)

				# wait till all of them send hashed response for caught up message

				if (await(len(self.caughtup_hash) == len(catch_up_replicas))):
					continue
				elif(timeout(10)):
					self.caughtup_hash = dict()
					log.info(self.INFO + "Catch up timeout, choosing next quorum")
					if (len(self.quorums) > 0):
						start_quorum_check()
					else:
						log.info(self.INFO + "No valid quorums found.")
						return

				# time.sleep(10)
				log.info(self.INFO + "Verifying if hash of all catchup is same.")
				if (all(x == list(self.caughtup_hash.values())[0] for x in self.caughtup_hash.values()) == True):
					send_pending_result_to_client(quorum)
					get_running_state(quorum)

				else:
					self.caughtup_hash = dict()
					log.info(self.INFO + "Catch up failed, choosing next quorum")
					if (len(self.quorums) > 0):
						start_quorum_check()
					else:
						log.info(self.INFO + "No valid quorums found.")
			else:
				log.info(self.INFO + "No need to catch up, lets just get the latest result for all clients")
				# If we are not sending catch up, then we need latest results that has to be sent to client
				
				if (self.init_config[TESTCASE_NAME]=="test_catchup"):
					log.info(self.INFO + "For the case of testcase, lets send catchup")

					for replica in quorum:
						log.info(self.INFO + "Sending catch up to " + str(replica))
						send(("catch_up", self.wedged_data[longest_replica].history), to=replica)

					if (await(len(self.caughtup_hash) == len(quorum))):
						continue
					elif(timeout(10)):
						self.caughtup_hash = dict()
						log.info(self.INFO + "Catch up timeout, choosing next quorum")
						if (len(self.quorums) > 0):
							start_quorum_check()
							return
						else:
							log.info(self.INFO + "No valid quorums found.")
							return

				send(("get_latest_result"), to=longest_replica)
				await(len(self.latest_result) > 0)
				send_pending_result_to_client([longest_replica])
				get_running_state(quorum)

		else:
			await(len(self.wedged_data) >= 3)
			if (len(self.quorums) > 0):
				log.info(self.INFO + " checking next quorum.")
				start_quorum_check()
			else:
				print('-----------------')
				log.info(self.INFO + "No valid quorums found.")


	# This method will send required details to the client
	def send_client_config(client):
		conf = ClientConfig(self.verify_keys, self.replica_list, int(self.init_config[TOLERENCE]))
		conf.head_id = self.head
		conf.tail_id = self.tail
		log.info(self.INFO + "Sending replica details to client " + str(client))
		send(("client_config", conf), to=client)

	# Send a message to all replicas asking for wedged object.
	# This message also tells replicas to become immutable
	def get_wedged_data():
		log.info(self.INFO + "Requesting wedged data and also asking replicas to become immutable.")
		for replica in self.replica_list:
			send(("wedged"), to=replica)

	################### Receivers for olympus ######################

	# Receiver for registering client with olympus.
	def receive(msg=("register_client", client_id, client_key), from_=c):
		log.info(self.INFO + "Received request to register client : " + str(client_id))
		self.clients[client_id] = client_key
		self.client_list.append(client_id)
		await(self.all_replicas_spawned == True)
		send_client_config(c)

	def receive(msg=("new_client_config", client_id), from_=c):
		send_client_config(c)

	def receive(msg=("is_config_changed", config_id), from_=c):
		log.info(self.INFO + "Received config change check request from client " + str(c))
		is_changed = False
		if (self.active_config != config_id):
			is_changed = True

		send(("config_change_result", is_changed, self.active_config), to=c)


	# If sender is client then we need to verify proof
	def receive(msg=("reconfigure", proof), from_=sender):

		# Verifying 
		if ("client" in str(sender).lower()):
			log.info(self.INFO + "Received reconfig request from a client, along with the proof. Let us verify the proof first.")
			if (verify_client_proof(proof)):
				log.info(self.INFO + "Proof of misbehaviour verified, let us start the process of reconfig.")
			else:
				return

		if (self.is_config_in_progress == False):
			self.is_config_in_progress = True
			log.info(self.INFO + "Received reconfig request from " + str(sender))
			get_wedged_data()

	# Wedged data from the replicas are receives here. As soon as we recieve t+1 data
	# we start verifying if quorum is valid. Meantime as and when we receive new data
	# we make quorums.

	def receive(msg=("wedged_data", wedged_data), from_=r):
		log.info(self.INFO + "Received wedged data from " + str(r))
		self.wedged_data[r] = wedged_data
		if (wedged_data.slot >= self.current_slot):
			self.current_slot = wedged_data.slot

		# As soon as we receive t+1 th wedged data we start making quorums
		if (len(self.wedged_data) >= float(self.init_config[TOLERENCE]) + 1):
			temp = []
			temp.extend(combinations(list(self.wedged_data.keys()), int(self.init_config[TOLERENCE]) + 1))
			# As soon as we receive t+1, they will be our first quorum
			for item in temp:
				self.quorums.add(item)

			# Print and call this methof for the first time.
			if (len(self.wedged_data) == float(self.init_config[TOLERENCE]) + 1):
				log.info(self.INFO + "Received t+1 wedge responses, starts making quorums.")
				start_quorum_check()

	# After catching up we receive hashed data here, this verification is to 
	# check if catchup was successful.

	def receive(msg=("catch_up_verification", hash_data, latest_result), from_=r):
		log.info(self.INFO + "Received catch up result from " + str(r))
		self.caughtup_hash[r] = hash_data
		self.latest_result[r] = latest_result

	def receive(msg=("running_state_data", data), from_=r):
		log.info(self.INFO + "Received running state from "+ str(r))
		print(data)
		self.running_state_data[r] = data

	def receive(msg=("latest_result", latest_result), from_=r):
		self.latest_result[r] = latest_result